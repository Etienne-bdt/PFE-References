@inproceedings{aziziCoupledVAEInterpolator2024,
  title = {Coupled {{VAE}} and {{Interpolator Approach}} for {{Fast Hyperspectral Image Emulation}}},
  booktitle = {2024 14th {{Workshop}} on {{Hyperspectral Imaging}} and {{Signal Processing}}: {{Evolution}} in {{Remote Sensing}} ({{WHISPERS}})},
  author = {Azizi, Chedly Ben and Guilloteau, Claire and Roussel, Gilles and Puigt, Matthieu},
  year = {2024},
  pages = {1--5},
  publisher = {IEEE},
  note = {Cocorico, ce papier parle d'un VAE o{\`u} l'on remplace l'encodeur par un r{\'e}seau 'interpolateur' qui vient projeter des variables biophysiques dans l'espace latent apr{\`e}s avoir entrain{\'e} le VAE pour de la reconstruction. Le d{\'e}codeur est frozen pour la deuxi{\`e}me partie.}
}

@misc{blumenstiel2025TerraMesh,
  title = {{{TerraMesh}}: {{A Planetary Mosaic}} of {{Multimodal Earth Observation Data}}},
  shorttitle = {{{TerraMesh}}},
  author = {Blumenstiel, Benedikt and Fraccaro, Paolo and Marsocci, Valerio and Jakubik, Johannes and Maurogiovanni, Stefano and Czerkawski, Mikolaj and Sedona, Rocco and Cavallaro, Gabriele and Brunschwiler, Thomas and {Bernabe-Moreno}, Juan and Long{\'e}p{\'e}, Nicolas},
  year = {2025},
  month = apr,
  number = {arXiv:2504.11172},
  eprint = {2504.11172},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.11172},
  urldate = {2025-05-12},
  abstract = {Large-scale foundation models in Earth Observation can learn versatile, label-efficient representations by leveraging massive amounts of unlabeled data. However, existing public datasets are often limited in scale, geographic coverage, or sensor variety. We introduce TerraMesh, a new globally diverse, multimodal dataset combining optical, synthetic aperture radar, elevation, and land-cover modalities in an Analysis-Ready Data format. TerraMesh includes over 9 million samples with eight spatiotemporal aligned modalities, enabling large-scale pre-training and fostering robust cross-modal correlation learning. We provide detailed data processing steps, comprehensive statistics, and empirical evidence demonstrating improved model performance when pre-trained on TerraMesh. The dataset will be made publicly available with a permissive license.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/disc/e.bardet/Zotero/storage/VMPDSQVG/Blumenstiel et al. - 2025 - TerraMesh A Planetary Mosaic of Multimodal Earth Observation Data.pdf}
}

@inproceedings{dortaStructuredUncertaintyPrediction2018,
  title = {Structured Uncertainty Prediction Networks},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Dorta, Garoe and Vicente, Sara and Agapito, Lourdes and Campbell, Neill DF and Simpson, Ivor},
  year = {2018},
  pages = {5477--5485},
  note = {Am{\'e}lioration des VAE en apprenant une variance plus complexe et creuse en utilisant cholesky. Cela am{\'e}liore les {\'e}chantillons mais au prix d'une petite quantit{\'e} de calculs en plus. Il est aussi propos{\'e} une fa{\c c}on plus performante {\`a} l'aide d'une distance de mahalanobis et de normalisation spectrale : regarder la distance entre la classe la plus proche et la pr{\'e}diction (pas pertinent ici)}
}

@article{dumeurSelfsupervisedSpatiotemporalRepresentation2024,
  title = {Self-Supervised Spatio-Temporal Representation Learning of {{Satellite Image Time Series}}},
  author = {Dumeur, Iris and Valero, Silvia and Inglada, Jordi},
  year = {2024},
  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  volume = {17},
  pages = {4350--4367},
  publisher = {IEEE},
  note = {Mod{\`e}le inspir{\'e} de BERT. U-BARN {$\rightarrow$} Encodeur Spatial et Spectral. L'image est divis{\'e}e en patches qui sont encod{\'e}s puis reconstruits par un U-NET. Les sorties des Unets sont ensuite embedd{\'e}es par position pour {\^e}tre ensuite donn{\'e}es {\`a} un transformer. L'id{\'e}e est d'ensuite d'ajouter une t{\^a}che (classif/segmentation, reconstruction) {\`a} la fin du U-BARN pour pouvoir l'entrainer {\`a} une t{\^a}che pr{\'e}cise. Probl{\`e}me : pas un VAE, les repr{\'e}sentations sont un espace de petite dimension mais pas probabiliste. Quid des incertitudes ? Autre question : les patches introduisent une limite dans la spatialit{\'e} de la donn{\'e}e, on reste local. Le c{\^o}t{\'e} temporel traverse les patchs embeddings : chaque instant de prise de vue passe dans un SSE. On reshape les SITS en b*t,c,h,w puis en b x w x h ,t ,d avant le transformer}
}

@article{gatopoulos2020Superresolution,
  title = {Super-Resolution Variational Auto-Encoders},
  author = {Gatopoulos, Ioannis and Stol, Maarten and Tomczak, Jakub M},
  year = {2020},
  journal = {arXiv preprint arXiv:2006.05218},
  eprint = {2006.05218},
  archiveprefix = {arXiv},
  note = {Construit sur {\textbackslash}citesohn2015Learning. Utilisation d'une image LR pour sampler un premier {\'e}tage de variables latentes u, puis samplage de z avec u et y et finalement x {\textbar} z. Papier qui n'entraine que sur des imagettes de 32*32 {\textbar} 64*64 car peu de puissance de calcul. Possibilit{\'e} d'am{\'e}lioration en 256*256 ??},
  file = {/home/disc/e.bardet/Zotero/storage/G558753H/Gatopoulos et al. - 2020 - Super-resolution Variational Auto-Encoders.pdf}
}

@article{gatopoulosSelfSupervisedVariationalAutoEncoders2021,
  title = {Self-{{Supervised Variational Auto-Encoders}}},
  author = {Gatopoulos, Ioannis and Tomczak, Jakub M.},
  year = {2021},
  month = jun,
  journal = {Entropy},
  volume = {23},
  number = {6},
  pages = {747},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e23060747},
  urldate = {2025-04-24},
  abstract = {Density estimation, compression, and data generation are crucial tasks in artificial intelligence. Variational Auto-Encoders (VAEs) constitute a single framework to achieve these goals. Here, we present a novel class of generative models, called self-supervised Variational Auto-Encoder (selfVAE), which utilizes deterministic and discrete transformations of data. This class of models allows both conditional and unconditional sampling while simplifying the objective function. First, we use a single self-supervised transformation as a latent variable, where the transformation is either downscaling or edge detection. Next, we consider a hierarchical architecture, i.e., multiple transformations, and we show its benefits compared to the VAE. The flexibility of selfVAE in data reconstruction finds a particularly interesting use case in data compression tasks, where we can trade-off memory for better data quality and vice-versa. We present the performance of our approach on three benchmark image data (Cifar10, Imagenette64, and CelebA).},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {deep generative modeling,deep learning,non-learnable transformations,probabilistic modeling},
  file = {/home/disc/e.bardet/Zotero/storage/JDBNNI8D/Gatopoulos et Tomczak - 2021 - Self-Supervised Variational Auto-Encoders.pdf}
}

@misc{jakubik2025TerraMind,
  title = {{{TerraMind}}: {{Large-Scale Generative Multimodality}} for {{Earth Observation}}},
  shorttitle = {{{TerraMind}}},
  author = {Jakubik, Johannes and Yang, Felix and Blumenstiel, Benedikt and Scheurer, Erik and Sedona, Rocco and Maurogiovanni, Stefano and Bosmans, Jente and Dionelis, Nikolaos and Marsocci, Valerio and Kopp, Niklas and Ramachandran, Rahul and Fraccaro, Paolo and Brunschwiler, Thomas and Cavallaro, Gabriele and {Bernabe-Moreno}, Juan and Long{\'e}p{\'e}, Nicolas},
  year = {2025},
  month = apr,
  number = {arXiv:2504.11171},
  eprint = {2504.11171},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.11171},
  urldate = {2025-04-29},
  abstract = {We present TerraMind, the first any-to-any generative, multimodal foundation model for Earth observation (EO). Unlike other multimodal models, TerraMind is pretrained on dual-scale representations combining both token-level and pixel-level data across modalities. On a token level, TerraMind encodes high-level contextual information to learn cross-modal relationships, while on a pixel level, TerraMind leverages fine-grained representations to capture critical spatial nuances. We pretrained TerraMind on nine geospatial modalities of a global, large-scale dataset. In this paper, we demonstrate that (i) TerraMind's dual-scale early fusion approach unlocks a range of zero-shot and few-shot applications for Earth observation, (ii) TerraMind introduces ``thinking in modalities'' (TiM)---the capability of generating additional artificial data during finetuning and inference to improve the model output---and (iii) TerraMind achieves beyond state-of-the-art performance in community-standard benchmarks for EO like PANGAEA. The pretraining dataset, the model weights, and our code will be open-sourced under a permissive license.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/disc/e.bardet/Zotero/storage/U9FJVZKX/Jakubik et al. - 2025 - TerraMind Large-Scale Generative Multimodality for Earth Observation.pdf}
}

@article{maud2024Deep,
  title = {Deep Priors for Satellite Image Restoration with Accurate Uncertainties},
  author = {Maud, Biquard and Chabert, Marie and Genin, Florence and Latry, Christophe and Oberlin, Thomas},
  year = {2024},
  journal = {arXiv preprint arXiv:2412.04130},
  eprint = {2412.04130},
  archiveprefix = {arXiv},
  note = {VBLE, utilisation d'une architecture CAE, proche d'un VAE qui estime un hyperprior {\`a} partir de {\textbackslash}barz. On peut donc sampler z {\`a} partir de l'hyperprior puis le d{\'e}coder pour obtenir la moyenne et la variance (en utilisant deux d{\'e}codeurs). Voir {\textbackslash}citerybkin2021simple concernant la m{\'e}thode. Comment mesurer incertitude ? Ici, pour un pixel, on sample 100 fois l'image et on regarde le pourcentage de fois o{\`u} la valeur du pixel est dans la valeur du nouveau sample {\textbackslash}pm5\% ?}
}

@inproceedings{prostEfficientPosteriorSampling2024,
  title = {Efficient {{Posterior Sampling For Diverse Super-Resolution}} with {{Hierarchical VAE Prior}}},
  booktitle = {{{VISAPP}} 2024-19th {{International Conference}} on {{Computer Vision Theory}} and {{Applications}}},
  author = {Prost, Jean and Houdard, Antoine and Almansa, Andr{\'e}s and Papadakis, Nicolas},
  year = {2024},
  note = {Dans ce papier, on a un VAE hi{\'e}rarchique conditionnel en quelque sorte. l'id{\'e}e est que les variables latentes donn{\'e}es par l'encodeur low-res captent les d{\'e}tails basse fr{\'e}quences et que conditionn{\'e} selon ca, on peut ensuite sampler les variables latentes pour ensuite sampler une version High-res en la passant dans notre mod{\`e}le g{\'e}n{\'e}ratif : \$\$. L'id{\'e}e est qu'on va contraindre les k premi{\`e}res variables latentes de l'encodeur HR et LR {\`a} correspondre en distribution (via une divergence KL) ce qui essaye de contraindre l'espace latent de capturer une repr{\'e}sentation BF/LR de l'image. Implem : tu prends un VDVAE et {\`a} chaque {\'e}tage tu viens calculer des moyennes et variances pour les var latentes ?}
}

@inproceedings{rybkinSimpleEffectiveVAE2021,
  title = {Simple and Effective {{VAE}} Training with Calibrated Decoders},
  booktitle = {International Conference on Machine Learning},
  author = {Rybkin, Oleh and Daniilidis, Kostas and Levine, Sergey},
  year = {2021},
  pages = {9179--9189},
  publisher = {PMLR},
  note = {Ici, l'id{\'e}e est de trouver des m{\'e}thodes plus faciles/efficaces d'utiliser un VAE. La m{\'e}thode cl{\'e} propos{\'e}e est par exemple de calculer la variance selon les donn{\'e}es et de sortir la moyenne via un r{\'e}seau de neurones. NB : La revue par les pairs est relativement mitig{\'e}e sur l'int{\'e}r{\^e}t du papier.}
}

@misc{salimans2017PixelCNN,
  title = {{{PixelCNN}}++: {{Improving}} the {{PixelCNN}} with {{Discretized Logistic Mixture Likelihood}} and {{Other Modifications}}},
  shorttitle = {{{PixelCNN}}++},
  author = {Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P.},
  year = {2017},
  month = jan,
  number = {arXiv:1701.05517},
  eprint = {1701.05517},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1701.05517},
  urldate = {2025-04-24},
  abstract = {PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {\section{Annotations\\
(24/04/2025 10:32:50)}

\par
<< By choosing a simple continuous distribution for modeling {$\nu$} (like the logistic distribution as done by Kingma et al. (2016)) we obtain a smooth and memory efficient predictive distribution for x. Here, we take this continuous univariate distribution to be a mixture of logistic distributions which allows us to easily calculate the probability on the observed discretized value x, as shown in equation (2) >> (Salimans et al., 2017, p. 2) Utilisation de mixture de logistiques pour mod{\'e}liser la densit{\'e} de proba sur 0,255},
  file = {/home/disc/e.bardet/Zotero/storage/4FW3HITL/Salimans et al. - 2017 - PixelCNN++ Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modificati.pdf}
}

@article{sheMAGICModularAutoencoder2024,
  title = {{{MAGIC}}: {{Modular Auto-encoder}} for {{Generalisable Model Inversion}} with {{Bias Corrections}}},
  author = {She, Yihang and Atzberger, Clement and Blake, Andrew and Gualandi, Adriano and Keshav, Srinivasan},
  year = {2024},
  journal = {arXiv preprint arXiv:2405.18953},
  eprint = {2405.18953},
  archiveprefix = {arXiv},
  note = {Utilisation d'un AE classique o{\`u} le d{\'e}codeur est remplac{\'e} par un mod{\`e}le physique en pytorch. Adaptabilit{\'e} avec un VAE pour faire de la g{\'e}n{\'e}ration ? Probl{\`e}me, on perd la partie p(x{\textbar}z). R{\'e}seau lin{\'e}aire, pertinence ? Les variables ont plus de sens physique mais m{\^e}me perf qu'un AE classique sur ce probl{\`e}me. Cit{\'e} une fois {\`a} la r{\'e}daction de ce document.}
}

@article{sohnLearningStructuredOutput2015,
  title = {Learning Structured Output Representation Using Deep Conditional Generative Models},
  author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  year = {2015},
  journal = {Advances in neural information processing systems},
  volume = {28},
  note = {Conditionnement d'un VAE avec une entr{\'e}e suppl{\'e}mentaire (typiquement un label) pour conditionner le mod{\`e}le dans l'espace latent}
}

@inproceedings{vazhentsevUncertaintyEstimationTransformer2022,
  title = {Uncertainty Estimation of Transformer Predictions for Misclassification Detection},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Vazhentsev, Artem and Kuzmin, Gleb and Shelmanov, Artem and Tsvigun, Akim and Tsymbalov, Evgenii and Fedyanin, Kirill and Panov, Maxim and Panchenko, Alexander and Gusev, Gleb and Burtsev, Mikhail and others},
  year = {2022},
  pages = {8237--8252},
  note = {Estimation d'incertitudes d'un transformer en utilisant du dropout pendant l'inf{\'e}rence. En droppant quelques neurones, on peut venir sampler la distribution estim{\'e}e de la sortie et donc avoir une id{\'e}e de l'incertitude.}
}

@misc{yu2025SatelliteCalculator,
  title = {{{SatelliteCalculator}}: {{A Multi-Task Vision Foundation Model}} for {{Quantitative Remote Sensing Inversion}}},
  shorttitle = {{{SatelliteCalculator}}},
  author = {Yu, Zhenyu and Idris, Mohd Yamani Idna and Wang, Pei},
  year = {2025},
  month = apr,
  number = {arXiv:2504.13442},
  eprint = {2504.13442},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.13442},
  urldate = {2025-04-29},
  abstract = {Quantitative remote sensing inversion plays a critical role in environmental monitoring, enabling the estimation of key ecological variables such as vegetation indices, canopy structure, and carbon stock. Although vision foundation models have achieved remarkable progress in classification and segmentation tasks, their application to physically interpretable regression remains largely unexplored. Furthermore, the multi-spectral nature and geospatial heterogeneity of remote sensing data pose significant challenges for generalization and transferability. To address these issues, we introduce SatelliteCalculator, the first vision foundation model tailored for quantitative remote sensing inversion. By leveraging physically defined index formulas, we automatically construct a large-scale dataset of over one million paired samples across eight core ecological indicators. The model integrates a frozen Swin Transformer backbone with a prompt-guided architecture, featuring cross-attentive adapters and lightweight task-specific MLP decoders. Experiments on the Open-Canopy benchmark demonstrate that SatelliteCalculator achieves competitive accuracy across all tasks while significantly reducing inference cost. Our results validate the feasibility of applying foundation models to quantitative inversion, and provide a scalable framework for task-adaptive remote sensing estimation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/disc/e.bardet/Zotero/storage/AJXRS3ZK/Yu et al. - 2025 - SatelliteCalculator A Multi-Task Vision Foundation Model for Quantitative Remote Sensing Inversion.pdf}
}

@phdthesis{zerahPhysicsbasedDeepRepresentation2024,
  title = {Physics-Based Deep Representation Learning of Vegetation Using Optical Satellite Image Time Series},
  author = {Z{\'e}rah, Yo{\"e}l},
  year = {2024},
  school = {Universit{\'e} de Toulouse},
  note = {Utilisation d'un mod{\`e}le physique comme d{\'e}codeur d'un VAE pour donner de l'interpr{\'e}tabilit{\'e} aux variables latentes. Les variables latentes sont s{\'e}par{\'e}es en deux parties (vecteurs coup{\'e}s en deux) une partie d{\'e}codeur 'al{\'e}atoire', une partie mod{\`e}le physique d{\'e}terministe. Une contrainte de reconstruction, MCRL : {\textbackslash}mathcalL\_MCRL = -ln(p(x\_i{\textbar}z\_i)) permet notamment d'avoir une moyenne proche des x\_i et une variance {\textbackslash}hat{\textbackslash}sigma{$^{2}\_$}i qui repr{\'e}sente l'incertitude du d{\'e}codeur.}
}
